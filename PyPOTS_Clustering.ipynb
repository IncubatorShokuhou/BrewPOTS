{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“‘ Tutorials for PyPOTS Clustering Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“€ Preparing the **PhysioNet-2012** dataset for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 09:29:08 [INFO]: Loading the dataset physionet_2012 with TSDB (https://github.com/WenjieDu/Time_Series_Database)...\n",
      "2023-05-16 09:29:08 [INFO]: Starting preprocessing physionet_2012...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "Loaded successfully!\n",
      "dict_keys(['n_classes', 'n_steps', 'n_features', 'train_X', 'train_y', 'val_X', 'val_y', 'test_X', 'test_y', 'scaler'])\n"
     ]
    }
   ],
   "source": [
    "from pypots.data.generating import gene_physionet2012\n",
    "\n",
    "# Load the PhysioNet-2012 dataset, disable artificially-missing values for evaluation\n",
    "physionet2012_dataset = gene_physionet2012(artificially_missing=False)\n",
    "\n",
    "# Take a look at the generated PhysioNet-2012 dataset, you'll find that everything has been prepared for you,\n",
    "# data splitting, normalization, additional artificially-missing values for evaluation, etc.\n",
    "print(physionet2012_dataset.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the datasets for training, validating, and testing.\n",
    "import numpy as np\n",
    "\n",
    "# don't need validation set\n",
    "dataset_for_training = {\n",
    "    \"X\": np.concatenate([physionet2012_dataset['train_X'], physionet2012_dataset['val_X']], axis=0),\n",
    "    \"y\": np.concatenate([physionet2012_dataset['train_y'], physionet2012_dataset['val_y']], axis=0),\n",
    "}\n",
    "\n",
    "dataset_for_testing = {\n",
    "    \"X\": physionet2012_dataset['test_X'],\n",
    "    \"y\": physionet2012_dataset['test_y'],\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ An exmaple of **CRLI** for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 08:40:03 [INFO]: the trained model will be saved to tutorial_results/clustering/crli/20230516_T084003, the tensorboard file will be saved to tutorial_results/clustering/crli/20230516_T084003/tensorboard\n",
      "2023-05-16 08:40:03 [INFO]: Model initialized successfully with the number of trainable parameters: 1,546,820\n"
     ]
    }
   ],
   "source": [
    "from pypots.optim import Adam\n",
    "from pypots.clustering import CRLI\n",
    "\n",
    "# initialize the model\n",
    "crli = CRLI(\n",
    "    n_steps=physionet2012_dataset[\"n_steps\"],\n",
    "    n_features=physionet2012_dataset[\"n_features\"],\n",
    "    n_clusters=physionet2012_dataset[\"n_classes\"],\n",
    "    n_generator_layers=2,\n",
    "    rnn_hidden_size=256,\n",
    "    rnn_cell_type=\"GRU\",\n",
    "    decoder_fcn_output_dims=[256, 128],  # the output dimensions of layers in the decoder FCN.\n",
    "    # Here means there are 3 layers. Leave it to default as None will results in\n",
    "    # the FCN haveing only one layer.\n",
    "    batch_size=32,\n",
    "    epochs=10,  # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    # here we set patience=5 to early stop the training if the evaluting loss doesn't decrease for 5 epoches.\n",
    "    patience=3,\n",
    "                # You can leave it to defualt as None to disable early stopping.\n",
    "    # give the generator optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    G_optimizer=Adam(lr=1e-3),\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    # give the discriminator optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    D_optimizer=Adam(lr=1e-3),\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    # this argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    num_workers=0,\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed//ssadfsadf\n",
    "    device='cpu',  # just leave it to default, PyPOTS will automatically assign the best device for you.\n",
    "                  # Set it to 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices.\n",
    "    saving_path=\"tutorial_results/clustering/crli\",  # set the path for saving tensorboard files\n",
    "    model_saving_strategy=\"best\",  # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 08:42:54 [INFO]: epoch 0: training loss_generator 3.3691, train loss_discriminator 0.3934\n",
      "2023-05-16 08:45:22 [INFO]: epoch 1: training loss_generator 3.4669, train loss_discriminator 0.3709\n",
      "2023-05-16 08:48:00 [INFO]: epoch 2: training loss_generator 3.4688, train loss_discriminator 0.3631\n",
      "2023-05-16 08:50:24 [INFO]: epoch 3: training loss_generator 3.4712, train loss_discriminator 0.3591\n",
      "2023-05-16 08:50:24 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2023-05-16 08:50:24 [INFO]: Finished training.\n",
      "2023-05-16 08:50:24 [INFO]: Saved the model to tutorial_results/clustering/crli/20230516_T084003/CRLI.pypots.\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "crli.fit(train_set=dataset_for_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "crli_prediction = crli.cluster(dataset_for_testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing clustering metrics: \n",
      "RI: 0.7511829319593613, \n",
      "CP: 0.8548790658882403,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import cal_rand_index, cal_cluster_purity\n",
    "\n",
    "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "RI = cal_rand_index(crli_prediction, dataset_for_testing[\"y\"])\n",
    "CP = cal_cluster_purity(crli_prediction, dataset_for_testing[\"y\"])\n",
    "\n",
    "print(\"Testing clustering metrics: \\n\"\n",
    "      f'RI: {RI}, \\n'\n",
    "      f'CP: {CP}\\n'\n",
    "      )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ An exmaple of **VaDER** for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 09:29:35 [INFO]: the trained model will be saved to tutorial_results/clustering/vader/20230516_T092935, the tensorboard file will be saved to tutorial_results/clustering/vader/20230516_T092935/tensorboard\n",
      "2023-05-16 09:29:35 [INFO]: Model initialized successfully with the number of trainable parameters: 293,642\n"
     ]
    }
   ],
   "source": [
    "from pypots.optim import Adam\n",
    "from pypots.clustering import VaDER\n",
    "\n",
    "# initialize the model\n",
    "vader = VaDER(\n",
    "    n_steps=physionet2012_dataset[\"n_steps\"],\n",
    "    n_features=physionet2012_dataset[\"n_features\"],\n",
    "    n_clusters=physionet2012_dataset[\"n_classes\"],\n",
    "    rnn_hidden_size=128,\n",
    "    d_mu_stddev=2,\n",
    "    pretrain_epochs=20,\n",
    "    batch_size=32,\n",
    "    epochs=10,  # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    # here we set patience=5 to early stop the training if the evaluting loss doesn't decrease for 5 epoches.\n",
    "    patience=3,\n",
    "                # You can leave it to defualt as None to disable early stopping.\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    # this argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    num_workers=0,\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    device='cpu',  # just leave it to default, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it to 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices.\n",
    "    saving_path=\"tutorial_results/clustering/vader\",  # set the path for saving tensorboard files\n",
    "    model_saving_strategy=\"best\",  # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-16 09:41:52 [INFO]: epoch 0: training loss 0.8087\n",
      "2023-05-16 09:42:56 [INFO]: epoch 1: training loss 0.6093\n",
      "2023-05-16 09:44:01 [INFO]: epoch 2: training loss 0.6157\n",
      "2023-05-16 09:45:06 [INFO]: epoch 3: training loss 0.6193\n",
      "2023-05-16 09:46:11 [INFO]: epoch 4: training loss 0.6308\n",
      "2023-05-16 09:46:11 [INFO]: Exceeded the training patience. Terminating the training procedure...\n",
      "2023-05-16 09:46:11 [INFO]: Finished training.\n",
      "2023-05-16 09:46:11 [INFO]: Saved the model to tutorial_results/clustering/vader/20230516_T092935/VaDER.pypots.\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "vader.fit(train_set=dataset_for_training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "vader_prediction = vader.cluster(dataset_for_testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing clustering metrics: \n",
      "RI: 0.7517747893791342, \n",
      "CP: 0.8548790658882403,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import cal_rand_index, cal_cluster_purity\n",
    "\n",
    "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "RI = cal_rand_index(vader_prediction, dataset_for_testing[\"y\"])\n",
    "CP = cal_cluster_purity(vader_prediction, dataset_for_testing[\"y\"])\n",
    "\n",
    "print(\"Testing clustering metrics: \\n\"\n",
    "      f'RI: {RI}, \\n'\n",
    "      f'CP: {CP},\\n'\n",
    "      )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pypots-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
