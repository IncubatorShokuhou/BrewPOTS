{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“‘ Tutorials for PyPOTS Imputation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“€ Preparing the **PhysioNet-2012** dataset for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-05-17 00:01:28 [INFO]: Loading the dataset physionet_2012 with TSDB (https://github.com/WenjieDu/Time_Series_Database)...\n",
      "2023-05-17 00:01:28 [INFO]: Starting preprocessing physionet_2012...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset physionet_2012 has already been downloaded. Processing directly...\n",
      "Dataset physionet_2012 has already been cached. Loading from cache directly...\n",
      "Loaded successfully!\n",
      "dict_keys(['n_classes', 'n_steps', 'n_features', 'train_X', 'train_y', 'val_X', 'val_y', 'test_X', 'test_y', 'scaler', 'test_X_intact', 'test_X_indicating_mask', 'val_X_intact', 'val_X_indicating_mask'])\n"
     ]
    }
   ],
   "source": [
    "from pypots.data.generating import gene_physionet2012\n",
    "\n",
    "# Load the PhysioNet-2012 dataset\n",
    "physionet2012_dataset = gene_physionet2012(artificially_missing_rate=0.1)\n",
    "\n",
    "# Take a look at the generated PhysioNet-2012 dataset, you'll find that everything has been prepared for you,\n",
    "# data splitting, normalization, additional artificially-missing values for evaluation, etc.\n",
    "print(physionet2012_dataset.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the datasets for training, validating, and testing.\n",
    "\n",
    "dataset_for_training = {\n",
    "    \"X\": physionet2012_dataset['train_X'],\n",
    "}\n",
    "\n",
    "dataset_for_validating = {\n",
    "    \"X\": physionet2012_dataset['val_X'],\n",
    "    \"X_intact\": physionet2012_dataset['val_X_intact'],\n",
    "    \"indicating_mask\": physionet2012_dataset['val_X_indicating_mask'],\n",
    "}\n",
    "\n",
    "dataset_for_testing = {\n",
    "    \"X\": physionet2012_dataset['test_X'],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ An exmaple of **SAITS** for imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 00:01:40 [INFO]: the trained model will be saved to tutorial_results/imputation/saits/20230517_T000140\n",
      "2023-05-17 00:01:40 [INFO]: the tensorboard file will be saved to tutorial_results/imputation/saits/20230517_T000140/tensorboard\n",
      "2023-05-17 00:01:40 [INFO]: Model initialized successfully with the number of trainable parameters: 1,378,358\n"
     ]
    }
   ],
   "source": [
    "from pypots.optim import Adam\n",
    "from pypots.imputation import SAITS\n",
    "\n",
    "# initialize the model\n",
    "saits = SAITS(\n",
    "    n_steps=physionet2012_dataset['n_steps'],\n",
    "    n_features=physionet2012_dataset['n_features'],\n",
    "    n_layers=2,\n",
    "    d_model=256,\n",
    "    d_inner=128,\n",
    "    n_heads=4,\n",
    "    d_k=64,\n",
    "    d_v=64,\n",
    "    dropout=0.1,\n",
    "    attn_dropout=0.1,\n",
    "    diagonal_attention_mask=True,  # otherwise the original self-attention mechanism will be applied\n",
    "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it to 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices.\n",
    "    device='cpu',  \n",
    "    # set the path for saving tensorboard and trained model files \n",
    "    saving_path=\"tutorial_results/imputation/saits\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 00:02:05 [INFO]: epoch 0: training loss 0.7151, validating loss 0.3206\n",
      "2023-05-17 00:02:31 [INFO]: epoch 1: training loss 0.5164, validating loss 0.3038\n",
      "2023-05-17 00:02:55 [INFO]: epoch 2: training loss 0.4577, validating loss 0.2784\n",
      "2023-05-17 00:03:21 [INFO]: epoch 3: training loss 0.4181, validating loss 0.2638\n",
      "2023-05-17 00:03:46 [INFO]: epoch 4: training loss 0.3894, validating loss 0.2535\n",
      "2023-05-17 00:04:11 [INFO]: epoch 5: training loss 0.3730, validating loss 0.2445\n",
      "2023-05-17 00:04:36 [INFO]: epoch 6: training loss 0.3579, validating loss 0.2425\n",
      "2023-05-17 00:05:00 [INFO]: epoch 7: training loss 0.3490, validating loss 0.2413\n",
      "2023-05-17 00:05:24 [INFO]: epoch 8: training loss 0.3399, validating loss 0.2347\n",
      "2023-05-17 00:05:49 [INFO]: epoch 9: training loss 0.3331, validating loss 0.2330\n",
      "2023-05-17 00:05:49 [INFO]: Finished training.\n",
      "2023-05-17 00:05:49 [INFO]: Saved the model to tutorial_results/imputation/saits/20230517_T000140/SAITS.pypots.\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "saits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "saits_imputation = saits.impute(dataset_for_testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mean absolute error: 0.2344\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import cal_mae\n",
    "\n",
    "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "testing_mae = cal_mae(\n",
    "    saits_imputation, physionet2012_dataset['test_X_intact'], physionet2012_dataset['test_X_indicating_mask'])\n",
    "print(\"Testing mean absolute error: %.4f\" % testing_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ An exmaple of **Transformer** for imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 00:05:51 [INFO]: the trained model will be saved to tutorial_results/imputation/transformer/20230517_T000551\n",
      "2023-05-17 00:05:51 [INFO]: the tensorboard file will be saved to tutorial_results/imputation/transformer/20230517_T000551/tensorboard\n",
      "2023-05-17 00:05:51 [INFO]: Model initialized successfully with the number of trainable parameters: 7,938,597\n"
     ]
    }
   ],
   "source": [
    "from pypots.optim import Adam\n",
    "from pypots.imputation import Transformer\n",
    "\n",
    "# initialize the model\n",
    "transformer = Transformer(\n",
    "    n_steps=physionet2012_dataset['n_steps'],\n",
    "    n_features=physionet2012_dataset['n_features'],\n",
    "    n_layers=6,\n",
    "    d_model=512,\n",
    "    d_inner=256,\n",
    "    n_heads=4,\n",
    "    d_k=128,\n",
    "    d_v=128,\n",
    "    dropout=0.1,\n",
    "    attn_dropout=0,\n",
    "    ORT_weight=1,  # you can adjust the weight values of arguments ORT_weight\n",
    "    # and MIT_weight to make the SAITS model focus more on one task. Usually you can just leave them to the default values, i.e. 1.\n",
    "    MIT_weight=1,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it to 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices.\n",
    "    device='cpu',  \n",
    "    # set the path for saving tensorboard and trained model files \n",
    "    saving_path=\"tutorial_results/imputation/transformer\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 00:06:50 [INFO]: epoch 0: training loss 0.8075, validating loss 0.5036\n",
      "2023-05-17 00:07:48 [INFO]: epoch 1: training loss 0.6279, validating loss 0.4808\n",
      "2023-05-17 00:08:41 [INFO]: epoch 2: training loss 0.5863, validating loss 0.4693\n",
      "2023-05-17 00:09:33 [INFO]: epoch 3: training loss 0.5682, validating loss 0.4655\n",
      "2023-05-17 00:10:23 [INFO]: epoch 4: training loss 0.5607, validating loss 0.4644\n",
      "2023-05-17 00:11:14 [INFO]: epoch 5: training loss 0.5546, validating loss 0.4632\n",
      "2023-05-17 00:12:03 [INFO]: epoch 6: training loss 0.5503, validating loss 0.4617\n",
      "2023-05-17 00:12:53 [INFO]: epoch 7: training loss 0.5479, validating loss 0.4604\n",
      "2023-05-17 00:13:42 [INFO]: epoch 8: training loss 0.5463, validating loss 0.4605\n",
      "2023-05-17 00:14:32 [INFO]: epoch 9: training loss 0.5426, validating loss 0.4583\n",
      "2023-05-17 00:14:32 [INFO]: Finished training.\n",
      "2023-05-17 00:14:32 [INFO]: Saved the model to tutorial_results/imputation/transformer/20230517_T000551/Transformer.pypots.\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "transformer.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "transformer_imputation = transformer.impute(dataset_for_testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mean absolute error: 0.4655\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import cal_mae\n",
    "\n",
    "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "testing_mae = cal_mae(transformer_imputation,\n",
    "                      physionet2012_dataset['test_X_intact'], physionet2012_dataset['test_X_indicating_mask'])\n",
    "print(\"Testing mean absolute error: %.4f\" % testing_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ An exmaple of **BRITS** for imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 00:14:36 [INFO]: the trained model will be saved to tutorial_results/imputation/brits/20230517_T001436\n",
      "2023-05-17 00:14:36 [INFO]: the tensorboard file will be saved to tutorial_results/imputation/brits/20230517_T001436/tensorboard\n",
      "2023-05-17 00:14:36 [INFO]: Model initialized successfully with the number of trainable parameters: 239,344\n"
     ]
    }
   ],
   "source": [
    "from pypots.optim import Adam\n",
    "from pypots.imputation import BRITS\n",
    "\n",
    "# initialize the model\n",
    "brits = BRITS(\n",
    "    n_steps=physionet2012_dataset['n_steps'],\n",
    "    n_features=physionet2012_dataset['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # just leave it to default, PyPOTS will automatically assign the best device for you.\n",
    "    # Set it to 'cpu' if you don't have CUDA devices. You can also set it to 'cuda:0' or 'cuda:1' if you have multiple CUDA devices.\n",
    "    device='cpu',  \n",
    "    # set the path for saving tensorboard and trained model files \n",
    "    saving_path=\"tutorial_results/imputation/brits\", \n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 00:15:04 [INFO]: epoch 0: training loss 0.9379, validating loss 0.3447\n",
      "2023-05-17 00:15:25 [INFO]: epoch 1: training loss 0.7300, validating loss 0.3031\n",
      "2023-05-17 00:15:45 [INFO]: epoch 2: training loss 0.6804, validating loss 0.2873\n",
      "2023-05-17 00:16:04 [INFO]: epoch 3: training loss 0.6574, validating loss 0.2789\n",
      "2023-05-17 00:16:24 [INFO]: epoch 4: training loss 0.6424, validating loss 0.2716\n",
      "2023-05-17 00:16:44 [INFO]: epoch 5: training loss 0.6310, validating loss 0.2673\n",
      "2023-05-17 00:17:04 [INFO]: epoch 6: training loss 0.6217, validating loss 0.2649\n",
      "2023-05-17 00:17:25 [INFO]: epoch 7: training loss 0.6141, validating loss 0.2629\n",
      "2023-05-17 00:17:45 [INFO]: epoch 8: training loss 0.6075, validating loss 0.2598\n",
      "2023-05-17 00:18:06 [INFO]: epoch 9: training loss 0.6028, validating loss 0.2585\n",
      "2023-05-17 00:18:06 [INFO]: Finished training.\n",
      "2023-05-17 00:18:06 [INFO]: Saved the model to tutorial_results/imputation/brits/20230517_T001436/BRITS.pypots.\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "brits.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "brits_imputation = brits.impute(dataset_for_testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mean absolute error: 0.2576\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import cal_mae\n",
    "\n",
    "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "testing_mae = cal_mae(\n",
    "    brits_imputation, physionet2012_dataset['test_X_intact'], physionet2012_dataset['test_X_indicating_mask'])\n",
    "print(\"Testing mean absolute error: %.4f\" % testing_mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸš€ An exmaple of **M-RNN** for imputation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pypots.optim import Adam\n",
    "from pypots.imputation import MRNN\n",
    "from pypots.utils.metrics import cal_mae\n",
    "\n",
    "# initialize the model\n",
    "# initialize the model\n",
    "mrnn = MRNN(\n",
    "    n_steps=physionet2012_dataset['n_steps'],\n",
    "    n_features=physionet2012_dataset['n_features'],\n",
    "    rnn_hidden_size=128,\n",
    "    batch_size=32,\n",
    "    # here we set epochs=10 for a quick demo, you can set it to 100 or more for better performance\n",
    "    epochs=10,\n",
    "    # here we set patience=3 to early stop the training if the evaluting loss doesn't decrease for 3 epoches.\n",
    "    # You can leave it to defualt as None to disable early stopping.\n",
    "    patience=3,\n",
    "    # give the optimizer. Different from torch.optim.Optimizer, you don't have to specify model's parameters when\n",
    "    # initializing pypots.optim.Optimizer. You can also leave it to default. It will initilize an Adam optimizer with lr=0.001.\n",
    "    optimizer=Adam(lr=1e-3),\n",
    "    # this num_workers argument is for torch.utils.data.Dataloader. It's the number of subprocesses to use for data loading.\n",
    "    # Leaving it to default as 0 means data loading will be in the main process, i.e. there won't be subprocesses.\n",
    "    # You can increase it to >1 if you think your dataloading is a bottleneck to your model training speed\n",
    "    num_workers=0,\n",
    "    # Set it to None to use the default device (will use CPU if you don't have CUDA devices).\n",
    "    # You can also set it to 'cpu' or 'cuda' explicitly, or ['cuda:0', 'cuda:1'] if you have multiple CUDA devices.\n",
    "    device=None,\n",
    "    # set the path for saving tensorboard and trained model files\n",
    "    saving_path=\"tutorial_results/imputation/mrnn\",\n",
    "    # only save the best model after training finished.\n",
    "    # You can also set it as \"better\" to save models performing better ever during training.\n",
    "    model_saving_strategy=\"best\",\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 00:15:04 [INFO]: epoch 0: training loss 0.9379, validating loss 0.3447\n",
      "2023-05-17 00:15:25 [INFO]: epoch 1: training loss 0.7300, validating loss 0.3031\n",
      "2023-05-17 00:15:45 [INFO]: epoch 2: training loss 0.6804, validating loss 0.2873\n",
      "2023-05-17 00:16:04 [INFO]: epoch 3: training loss 0.6574, validating loss 0.2789\n",
      "2023-05-17 00:16:24 [INFO]: epoch 4: training loss 0.6424, validating loss 0.2716\n",
      "2023-05-17 00:16:44 [INFO]: epoch 5: training loss 0.6310, validating loss 0.2673\n",
      "2023-05-17 00:17:04 [INFO]: epoch 6: training loss 0.6217, validating loss 0.2649\n",
      "2023-05-17 00:17:25 [INFO]: epoch 7: training loss 0.6141, validating loss 0.2629\n",
      "2023-05-17 00:17:45 [INFO]: epoch 8: training loss 0.6075, validating loss 0.2598\n",
      "2023-05-17 00:18:06 [INFO]: epoch 9: training loss 0.6028, validating loss 0.2585\n",
      "2023-05-17 00:18:06 [INFO]: Finished training.\n",
      "2023-05-17 00:18:06 [INFO]: Saved the model to tutorial_results/imputation/brits/20230517_T001436/BRITS.pypots.\n"
     ]
    }
   ],
   "source": [
    "# train the model on the training set, and validate it on the validating set to select the best model for testing in the next step\n",
    "mrnn.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "mrnn_imputation = mrnn.impute(dataset_for_testing)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mean absolute error: 0.2576\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import cal_mae\n",
    "\n",
    "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "testing_mae = cal_mae(\n",
    "    mrnn_imputation, physionet2012_dataset['test_X_intact'], physionet2012_dataset['test_X_indicating_mask'])\n",
    "print(\"Testing mean absolute error: %.4f\" % testing_mae)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ An exmaple of **LOCF** for imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypots.imputation import LOCF\n",
    "\n",
    "# initialize the model\n",
    "locf = LOCF(\n",
    "    nan=0  # set the value used to impute data missing at the beginning of the sequence, those cannot use LOCF mechanism to impute\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/ml/lib/python3.9/site-packages/pypots/imputation/locf/model.py:63: UserWarning: LOCF (Last Observed Carried Forward) imputation class has no parameter to train. Please run func impute(X) directly.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LOCF doesn't need to be trained, just call the impute() function\n",
    "\n",
    "locf.fit(train_set=dataset_for_training, val_set=dataset_for_validating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the testing stage, impute the originally-missing values and artificially-missing values in the test set\n",
    "locf_imputation = locf.impute(dataset_for_testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing mean absolute error: 0.4110\n"
     ]
    }
   ],
   "source": [
    "from pypots.utils.metrics import cal_mae\n",
    "\n",
    "# calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "testing_mae = cal_mae(\n",
    "    locf_imputation, physionet2012_dataset['test_X_intact'], physionet2012_dataset['test_X_indicating_mask'])\n",
    "print(\"Testing mean absolute error: %.4f\" % testing_mae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
